---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "SDS348"
date: '2020-05-13'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

### Introduction

```{r cars}
credit <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Credit.csv")
head(credit)
```

The dataset I have chosen is the Credit Card Balance Data, which provides data surrounding credit cards as well as other pertinent customer information. More specifically, the original dataset includes 400 observations of 13 variables: ID (identification), income (in $10,000's), limit (credit limit), rating (credit rating), cards (number of credit cards), age (in years), education (number of years), gender (male or female), student (yes or no), married (yes or no), ethnicity (African American, Asian, or Caucasian), and Balance (average credit card balance in dollars).

```{r}
# Assumptions:
  # Multivariate normality
ggplot(credit, aes(x = Limit, y = Balance)) +
geom_point(alpha = .5) + geom_density_2d(h=2) + coord_fixed() + facet_wrap(~Student)

  #Homogeneity of covariances 
covmats<-credit%>%group_by(Student)%>%do(covs=cov(.[2:3]))
for(i in 1:2){print(as.character(covmats$Student[i])); print(covmats$covs[i])}

# MANOVA
manproj <-manova(cbind(Limit, Balance) ~ Student, data= credit)
summary(manproj)
summary.aov(manproj)


1-.95^3  # Type 1 Error 
0.05/3 # Bonferroni  
```

A one-way MANOVA was conducted to determine the effect of Student (Yes or No) on two dependent variables (Balance and Limit). I first checked the assumptions of multivariate normality of DVs and homogeneity of within-group covariance matrices. When testing for multivariate normality of DVs, it was hard to tell if that assumption was met but just from eyeballing it, I could not see any distinct departures. For homogeneity, some values in the covariance matrices were similar while others were not (but overall relative homogeneity). 

In total, I ran 3 tests: 1 MANOVA and 2 univariate ANOVAs. I did not run a post-hoc t test since my response variable is binary. To keep the overall type I error rate at 0.05, the boneferroni adjusted signficance level is 0.01666667. After running a MANOVA, I got a p-value of < 0.00000000000000022, Pillai of 0.27114, and a F(2, 397) of 73.841 which means that at least one of my response variables (Balance or Limit) significantly differs by Student status (whether the individual was a student or not). Then I ran 2 univariate ANOVAs and determined that only Balance significantly differs by Student status with a p-value of 0.0000001488 and F(1, 398) value of 28.622. The probability of making at least one type I error (if left unadjusted) is 0.142625. 

### Randomization test

```{r}

library(dplyr)
set.seed(348)
head(credit)
credit%>%group_by(Married)%>%
  summarize(means=mean(Rating))%>%summarize(`mean_diff:`=diff(means))

rand_distp<-vector()

for(i in 1:5000){
newp<-data.frame(rating=sample(credit$Rating), married=credit$Married) 
rand_distp[i]<-mean(newp[newp$married=="Yes",]$rating)-
              mean(newp[newp$married=="No",]$rating)
}

mean(rand_distp>11.65714 | rand_distp< -11.65714)

# Plot
{hist(rand_distp,main="",ylab=""); abline(v = 11.65714,col="red")}

```

I performed a randomization test on my dataset in order to determine if there is a mean difference in credit rating between individuals that are married and those that are unmarried. The null hypothesis is that the mean credit rating is the same for married individuals vs. unmarried individuals. The alternative hypothesis is that the mean credit rating is different for married vs. unmarried individuals. After running my randomization test, I got a p-value of 0.4688. Since this p-value is greater than the alpha value of 0.05, I fail to reject the null hypothesis. In other words, there is no mean difference in credit rating for married vs. unmarried individuals. 

### Linear Regression Model

```{r}
# Linear Regression 
credit$Rating_c<-credit$Rating-mean(credit$Rating, na.rm = "T")
credit$Income_c<-credit$Income-mean(credit$Income, na.rm = "T")


p3fit <- lm(Balance ~ Income_c*Rating_c, data= credit)
summary(p3fit)

# Plot
p3fit <- lm(Balance ~ Income_c*Rating_c, data= credit)
new1<-credit
new1$Income_c<-mean(credit$Income_c)
new1$mean<-predict(p3fit,new1)
new1$Income_c<-mean(credit$Income_c)+sd(credit$Income_c)
new1$plus.sd<-predict(p3fit,new1)
new1$Income_c<-mean(credit$Income_c)-sd(credit$Income_c)
new1$minus.sd<-predict(p3fit,new1)


mycols<-c("#619CFF","#F8766D","#00BA38")
names(mycols)<-c("-1 sd","mean","+1 sd")
mycols=as.factor(mycols)

ggplot(credit,aes(Rating_c,Balance),group=mycols)+geom_point()+geom_line(data=new1,aes(y=mean,color="mean"))+geom_line(data=new1,aes(y=plus.sd,color="+1 sd"))+geom_line(data=new1,aes(y=minus.sd,color="-1 sd"))+scale_color_manual(values=mycols)+labs(color="Income (cont)")+theme(legend.position=c(.9,.2))

# Assumptions:
library(sandwich); library(lmtest)
presids<-p3fit$residuals; pfitvals<-p3fit$fitted.values 
ggplot()+geom_point(aes(pfitvals,presids))+geom_hline(yintercept=0, col="red") #linearity looks iffy but okay
bptest(p3fit)  # fail to reject null hypothesis of homoskedasticity
shapiro.test(presids) # reject null hypothesis of normally distributed residuals
ggplot()+geom_histogram(aes(presids),bins=20) # can see residuals are skewed to the right!


coeftest(p3fit, vcov = vcovHC(p3fit)) #robust SE

# Multiple R-squared =  0.8777


```
After running a linear regression on my data, I got the following interpretations. Intercept: predicted Balance for an average income and rating is 505.403507. Income_c: At an average rating, individuals show a decrease of 8.398708 in balance for every 1-unit increase in income on average. Rating_c: At an average income, individuals show an increase of 3.948726 in balance for every 1-unit increase in rating on average. Income_c:Rating_c: For every one unit increase in rating, the slope for income gets 0.00442 larger.  

I checked the assumptions for a linear regression and found that linearity looked okay, homoskedasticity was met (p-value: 0.8564), but my residuals were not normally distributed (p-value: <0.00000000000000022). I made a plot of my residuals and could see that it skewed to the right. I then recomputed the regression results with robust standard errors via 'coeftest(...,vcov=vcovHC(...))'. Income_c, Rating_c, and Income_c:Rating_c were all significant with p-values of < 0.00000000000000022, < 0.00000000000000022, and 0.001922 respectively. In other words, all these variables are significant predictors of Balance. The p-value for the interaction term changed from 0.00442 to 0.001922 with the robust SEs. Before computing the robust SEs, my SEs were  9.549534, 0.452875, 0.085438, and 0.001186 for the intercept, Income_c, Rating_c, and Income_c:Rating_c respectively. After computing the robust SEs, my SEs were 9.6332553, 0.4765486, 0.0825803, 0.0010869 for the intercept, Income_c, Rating_c, and Income_c:Rating_c respectively. The SEs increased for the intercept and Income_c after computing the robust SEs but decreased for Rating_c and Income_c:Rating_c. The proportion of variation in the outcome my model explains is 0.8777 (multiple r-squared value).


### Linear Regression Model with Bootstrapped SEs

```{r}
# Bootstrapped SEs (resampling rows)
samp_distn<-replicate(5000, {
boot_dat<-boot_dat<-credit[sample(nrow(credit),replace=TRUE),]
bootp3fit <- lm(Balance ~ Income_c*Rating_c, data= boot_dat)
coef(bootp3fit)
})

# Estimated SEs
samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)

# Bootstrapped SEs (resampling residuals)
p3fit <- lm(Balance ~ Income_c*Rating_c, data= credit)
presids<-p3fit$residuals
pfitvals<-p3fit$fitted.values
resid_resamp<-replicate(5000,{
new_resids<-sample(presids,replace=TRUE)
newdat<-credit
newdat$new_y<-pfitvals+new_resids
newp3fit<-lm(new_y ~ Income_c*Rating_c, data = newdat)
coef(newp3fit)
})

# Estimated SEs
resid_resamp%>%t%>%as.data.frame%>%summarize_all(sd)

# Comparisons
coeftest(p3fit)[,1:2] # Normal-theory SEs
coeftest(p3fit, vcov=vcovHC(p3fit))[,1:2] # Robust SEs
samp_distn%>%t%>%as.data.frame%>%summarize_all(sd) # Bootstrapped SEs (resampling rows)
resid_resamp%>%t%>%as.data.frame%>%summarize_all(sd) # Bootstrapped SEs (resampling residuals)
```
The Bootstrapped SEs from resampling rows for Intercept, Income_c, Rating_c, and Income_c:Rating_c were 9.5378, 0.4787259, 0.08331522, and 0.0010756 respectively. The Bootstrapped SEs from resampling residuals for Intercept, Income_c, Rating_c, and Income_c:Rating_c were 9.546869, 0.4545284, 0.08450937, and	0.001180758 respectively. The original SEs for Intercept, Income_c, Rating_c, and Income_c:Rating_c were 9.549534, 0.452875, 0.085438, and 0.001186 with p-values of < 0.0000000000000002, < 0.0000000000000002, < 0.0000000000000002, and 0.00442 respectively. The robust SEs for Intercept, Income_c, Rating_c, and Income_c:Rating_c were 9.6332553, 0.4765486, 0.0825803, and 0.0010869 with p-values of < 0.00000000000000022, < 0.00000000000000022, < 0.00000000000000022, and 0.001922 respectively.

When looking at SEs: The SEs are all around the same value for each respective variable, give or take a very small increase or decrease in value between the different types of SEs. When looking at p-values: The p-values for Intercept, Income_c, and Rating_c were the same for both robust SEs and the original SEs. However, the p-value for the interaction of Income_c:Rating_c decreased from 0.00442 to 0.001922 for the robust SEs. 

### Logistic Regression
    
```{r}
library(tidyverse)
library(lmtest)

class_diag <- function(probs,truth){
#CONFUSION MATRIX: CALCULATE ACCURACY, TPR, TNR, PPV
tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
#CALCULATE EXACT AUC
ord<-order(probs, decreasing=TRUE)
probs <- probs[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,auc)
} 

# Logistic Regression
logfitp <- glm(Student ~ Balance + Cards, data = credit, family = binomial(link = "logit"))
coeftest(logfitp)
exp(coef(logfitp))
probproj <-predict(logfitp,type="response")

# Confusion matrix
table(predict=as.numeric(probproj>.5),truth=credit$Student)%>%addmargins
class_diag(probproj, credit$Student) %>% round(4)

# Accuracy 
(359+0)/400
# Sensitivity (TPR)
0/40
# Specificity (TNR)
359/360
# Recall (PPV)
0/1
# Density of log-odds plot
credit$logit <-predict(logfitp, type = "link") #get predicted log-odds
ggplot(credit,aes(logit, fill=Student, color = Student))+geom_density(alpha=.4)+
  geom_vline(xintercept=0,lty=2)

# ROC curve
library(plotROC)
ROCcurve<- ggplot(credit)+geom_roc(aes(d=Student, m=probproj), n.cuts=0)
ROCcurve
calc_auc(ROCcurve)

# 10 fold CV
set.seed(1234)
k=10
pdata<-credit[sample(nrow(credit)),] #randomly order rows
folds<-cut(seq(1:nrow(credit)),breaks=k,labels=F) 

diagp<-NULL
for(i in 1:k){
## Create training and test sets
train<-pdata[folds!=i,]
test<-pdata[folds==i,]
truth<-test$Student
## Train model on training set
trainfit <- glm(Student ~ Balance + Cards, data = train, family = "binomial")
testprob <-predict(trainfit,newdata = test,type="response")
## Test model on test set (save all k results)
diagp<-rbind(diagp,class_diag(testprob,truth))
}
summarize_all(diagp,mean)  
```
I ran a logistic regression to model the probability of (being a) Student based on the explanatory variables of Balance and Cards. For the intercept: odds of (being a) Student when Balance= 0 and Cards=0, is  0.04846555. For Balance: Controlling for Cards, for every one unit increase in Balance, odds of (being a) Student increase by a factor of 1.00182360 (significant). For Cards: Controlling for Balance, for every one unit increase in Cards, odds of (being a) Student increase by a factor 0.87251775 (not significant). After running a confusion matrix, I got the following: accuracy= 0.8975 (proportion of correctly classified cases), sensitivity= 0 (proprotion of students correctly classified), specificity= 0.9972 (proportion of those that are not students correctly classified), ppv= 0 (proportion classified as a student who actually are). From the ROC curve generated, I calculated the AUC to be 0.7349653. The AUC is the probability that a randomly selected person that is a student has a higher predicted probability than a randomly selected person that is not a student. This AUC value is fair. I then performed a 10-fold CV and got the following out-of-sample Accuracy, Sensitivity, Recall, and AUC respectively: 0.895, 0, NaN, and 0.7192716.


### LASSO

```{r}
library(dplyr)
ncredit <- credit %>% dplyr::select(-Rating_c, -Income_c, -logit)
# 10-fold CV with linear regression
set.seed(1234)
k=10 #choose number of folds
data1<-ncredit[sample(nrow(ncredit)),] #randomly order rows
folds<-cut(seq(1:nrow(ncredit)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
train<-data1[folds!=i,]
test<-data1[folds==i,]
fit<-lm(Balance~.,data=train)
yhat<-predict(fit,newdata=test)
diags<-mean((test$Balance-yhat)^2)
}
mean(diags)

# LASSO regression
library(glmnet)
data(ncredit)
y <- as.matrix(ncredit$Balance)
x <- model.matrix(Balance~.,data=ncredit)[,-1]
x <- scale(x)
head(x)
cv<-cv.glmnet(x,y)
lasso1<-glmnet(x,y,lambda=cv$lambda.1se)
coef(lasso1)

# LASSO 10-fold CV
ncredit$Yes <- ifelse(ncredit$Student == "Yes",1,0)
set.seed(1234)
k=10 #choose number of folds
data1<-ncredit[sample(nrow(ncredit)),] #randomly order rows
folds<-cut(seq(1:nrow(ncredit)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
train<-data1[folds!=i,]
test<-data1[folds==i,]
fit<-lm(Balance~Income+Limit+Rating+Cards+Age+Yes,data=train)
yhat<-predict(fit,newdata=test)
diags<-mean((test$Balance-yhat)^2)
}
mean(diags)
```
I chose Balance to predict and run a LASSO regression inputting all the rest of my variables as predictors. I first ran a 10-fold CV on my full model and got a MSE of 10957.86. Then I used LASSO for variable selection and found that Income, Limit, Rating, Cards, Age, and Student.Yes are the most important predictors of Balance. I then performed a 10-fold CV using these predictors. After doing so, I got a RMSE of 10944.67. When just comparing the CV MSE for the full model to the LASSO-variables CV MSE, the MSE dropped from 10957.86 with the full model to 10944.67 with the LASSO-variables. While it is not that big of a difference, the LASSO-variables model predicts slightly better. 
    

